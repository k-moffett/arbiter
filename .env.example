# Arbiter Environment Configuration
# Copy this file to .env and update with your values

# ==================================================================
# Logging Configuration
# ==================================================================
# Log prefix to differentiate Arbiter logs from Ollama/other services
LOG_PREFIX=[ARBITER]

# Log format: 'text' for human-readable, 'json' for structured logging
# Use 'json' for parsing with jq or log aggregators
LOG_FORMAT=text

# Log level: DEBUG, INFO, WARN, ERROR, FATAL
LOG_LEVEL=INFO

# Enable colored output in console logs (true/false)
LOG_USE_COLORS=true

# Force console logger even in production (true/false)
LOG_TO_CONSOLE=false

# ==================================================================
# LLM Configuration
# ==================================================================
# LLM model for chat/completions
# Available models (ordered by capability and VRAM requirements):
#   llama3.1:70b-instruct-q4_K_M - Most powerful, requires 48GB+ VRAM
#   qwen2.5:32b                  - Smarter than llama3.1:8b, faster than 70b, requires 20-24GB VRAM
#   qwen2.5:14b                  - Best for 12GB VRAM (RTX 4070), excellent quality/performance balance (DEFAULT)
#   llama3.1:8b                  - Fast and efficient, requires ~8GB VRAM
#   qwen2.5:7b                   - Smaller and faster, requires ~7GB VRAM
LLM_MODEL=qwen2.5:14b

# Embedding model (used for vector search)
EMBEDDING_MODEL=nomic-embed-text

# Component-specific model overrides (optional, defaults to LLM_MODEL)
# Useful for using different model sizes for different RAG pipeline stages
# Example: Use 7b for fast validation, 14b for complex reasoning
QUERY_DECOMPOSER_MODEL=${LLM_MODEL}
QUERY_ENHANCER_MODEL=${LLM_MODEL}
RAG_VALIDATOR_MODEL=${LLM_MODEL}
QUALITY_GRADER_MODEL=${LLM_MODEL}
TOOL_PLANNER_MODEL=${LLM_MODEL}

# ==================================================================
# Ollama Configuration
# ==================================================================
# Ollama server URL
OLLAMA_BASE_URL=http://localhost:11434

# Ollama request timeout in milliseconds
OLLAMA_TIMEOUT=180000

# Enable Ollama debug logging (true/false)
# Note: Ollama logs are separate from Arbiter logs
OLLAMA_DEBUG=false

# Model warming - Pre-load models into memory after startup (true/false)
# This reduces first-request latency but increases startup time
# Recommended: true for production, false for development
OLLAMA_WARM_MODELS=true

# Ollama Model Loading Configuration
OLLAMA_NUM_PARALLEL=2           # Parallel inference requests (2 = embeddings + chat can run simultaneously)
OLLAMA_MAX_LOADED_MODELS=2      # Max models kept in VRAM (2 = embedding model + 1 LLM)
OLLAMA_FLASH_ATTENTION=1        # Enable Flash Attention for faster inference

# Ollama Memory Limits (Docker Deploy Resources)
# Adjust based on your GPU VRAM and models:
#   qwen2.5:14b: 4G reservation, 12G limit (recommended for RTX 4070 12GB)
#   qwen2.5:32b: 8G reservation, 24G limit (requires RTX 4090 24GB+)
OLLAMA_MEMORY_RESERVATION=4G
OLLAMA_MEMORY_LIMIT=12G

# ==================================================================
# Context Window Configuration
# ==================================================================
# Model context limits:
#   qwen2.5:14b: 16K tokens (use 12288 = 75% for safety)
#   qwen2.5:32b: 32K tokens (use 24576 = 75% for safety)
#   llama3.1:8b: 8K tokens (use 6144 = 75% for safety)
CONTEXT_MAX_TOKENS=12288        # Max tokens for retrieved context
CONTEXT_MIN_RESPONSE_TOKENS=512 # Reserved tokens for LLM response
CONTEXT_CHARS_PER_TOKEN=4       # Character-to-token estimation (upgrade to tiktoken later)

# ==================================================================
# RAG Performance Optimization
# ==================================================================
# Query Router Configuration
# Complexity threshold: queries with complexity >5 use complex path (with HyDE + expansion)
# Lower values = better retrieval quality, slightly higher latency
# Higher values = faster but less accurate retrieval
QUERY_ROUTER_COMPLEXITY_THRESHOLD=5    # Complex path for >5 (default: 5, was 7)

# Hybrid Search Configuration
HYBRID_SEARCH_MAX_RESULTS=20    # Reduce from 50 to 20 (fewer results = faster validation)

# RAG Validator Configuration
RAG_VALIDATOR_MAX_PARALLEL=15   # Validate 15 results at once (increased from 5)
RAG_VALIDATOR_MIN_SCORE=0.15    # More permissive threshold (decreased from 0.3)

# ==================================================================
# MCP Server Configuration
# ==================================================================
# MCP Server URL
# Use 'mcp-server' for Docker, 'localhost' for local development
MCP_SERVER_URL=http://localhost:3100

# Transport type: stdio or streamable-http
TRANSPORT=streamable-http

# HTTP port for MCP server
MCP_HTTP_PORT=3100

# Maximum concurrent requests
MAX_CONCURRENT_REQUESTS=50

# Request timeout in milliseconds
REQUEST_TIMEOUT=30000

# ==================================================================
# Qdrant Configuration
# ==================================================================
# Qdrant vector database URL
# Use 'qdrant' for Docker, 'localhost' for local development
QDRANT_URL=http://localhost:6333

# Optional Qdrant API key (leave empty for no auth)
QDRANT_API_KEY=

# ==================================================================
# Node.js Configuration
# ==================================================================
# Environment: development, production, local
NODE_ENV=development

# Node memory options
NODE_OPTIONS=--max-old-space-size=2048

# ==================================================================
# Agent Orchestrator Configuration
# ==================================================================
# Use 'agent-orchestrator' for Docker, 'localhost' for local development
AGENT_ORCHESTRATOR_URL=http://localhost:3200

# Orchestrator server port
ORCHESTRATOR_PORT=3200


# ==============================================================================
# CLI Customization
# ==============================================================================
# Customize the appearance and behavior of the interactive CLI client
# Usage: Set these variables before running: npm run cli

# New Western Brand
CLI_BANNER_FONT=Doom
CLI_WELCOME_TITLE=New Western AI
CLI_WELCOME_MESSAGE="Forge Your Path. Sharpen Your Edge."
CLI_USE_GRADIENT=true
CLI_GRADIENT_THEME=gold
AGENT_PERSONALITY=newwestern
PERSISTENT_AGENT_PERSONALITY=false

# Personal Brand (The Nerdy Oneâ„¢)
# Uncomment these to transform into your dice-hoarding, lore-obsessed AI companion
# Perfect for when you need answers with a +5 bonus to enthusiasm
#CLI_WELCOME_TITLE=The Dice Goblin Oracle
#CLI_WELCOME_MESSAGE="Rolling for Initiative... in Knowledge Retrieval! *Crit Success on Helping You*"
#AGENT_PERSONALITY=tabletop
#CLI_BANNER_FONT=ANSI Shadow
#CLI_GRADIENT_THEME=rainbow
# Pro tip: It knows WAY too much about Warhammer 40K lore. Like, concerningly too much.


# Session Statistics
# Show message count and average response time after each message (default: "true")
# Set to "false" to hide statistics for a cleaner interface
#CLI_SHOW_STATS=true

# Terminal Compatibility (PowerShell/WSL/Docker)
# Force color support in terminals that don't auto-detect
CLI_FORCE_COLOR=true

# Override terminal width detection (useful for Docker/WSL)
# Set to your actual terminal width or leave unset for auto-detect (default: 100)
# Using 80 for better compatibility with most terminals
CLI_TERMINAL_WIDTH=80

# ==============================================================================
# LLM Provider Configuration
# ==============================================================================
# Ollama request timeout in milliseconds (default: 30000)
# First request takes longer due to model loading (30+ seconds)
# Set to 90 seconds to prevent timeout on initial load
OLLAMA_TIMEOUT=90000

# Agent Orchestrator Client timeout in milliseconds (default: 60000)
# Total request time includes Ollama load time + MCP calls + orchestration
# Set to 120 seconds to handle first request with model loading
AGENT_ORCHESTRATOR_TIMEOUT=120000

# Examples:
# Custom branding:
#   CLI_WELCOME_TITLE=My Custom Bot
#   CLI_WELCOME_MESSAGE=Powered by Ollama & Qdrant
#
# Minimal mode (no gradients or stats):
#   CLI_USE_GRADIENT=false
#   CLI_SHOW_STATS=false

# ==================================================================
# Document Ingestion Configuration
# ==================================================================
# Default chunking strategy: simple (paragraph-based) or semantic (LLM-powered)
# - simple: Fast, rule-based chunking on paragraph boundaries
# - semantic: Intelligent chunking respecting semantic boundaries (requires Ollama)
DEFAULT_CHUNKING_STRATEGY=simple

# Require document metadata before ingestion (true/false)
# When true, title, author, description, tags, and category are required
# Metadata improves discoverability and enables better filtering
REQUIRE_DOCUMENT_METADATA=false

# Automatic metadata extraction using LLM (true/false)
# When true, metadata is automatically extracted from document content
# CLI flags can override any extracted field
AUTO_EXTRACT_METADATA=true

# Model for metadata extraction
# Smaller models work well for this task (7b-8b recommended)
# Use a model you have available (e.g., llama3.1:8b, qwen2.5:7b)
METADATA_EXTRACTION_MODEL=llama3.1:8b

# Number of pages to sample from document start for metadata analysis
# Balances accuracy vs. performance (3-10 recommended)
METADATA_EXTRACTION_SAMPLE_PAGES=5

# Temperature for metadata extraction (0-1)
# Lower = more deterministic, higher = more creative
# Use low values (0.1) for factual extraction
METADATA_EXTRACTION_TEMPERATURE=0.1

# ==================================================================
# Semantic Chunking Configuration (Ollama-Powered)
# ==================================================================
# Only used when chunking strategy is 'semantic'
# These settings control LLM-based boundary detection and metadata extraction

# Chunk Size Configuration
SEMANTIC_CHUNK_MIN_SIZE=300          # Minimum chunk size in characters
SEMANTIC_CHUNK_MAX_SIZE=1500         # Maximum chunk size (for non-atomic content)
SEMANTIC_CHUNK_TARGET_SIZE=1000      # Target/ideal chunk size
SEMANTIC_CHUNK_ATOMIC_MAX_SIZE=2000  # Max size for atomic units (tables, Q&A pairs)
SEMANTIC_CHUNK_OVERLAP_SIZE=225      # Overlap size for context preservation

# Model Configuration
# Base model for all semantic analyzers (required)
OLLAMA_SEMANTIC_CHUNKER_MODEL=llama3.2:3b

# Optional: Per-analyzer model overrides (falls back to base model if not set)
# Useful for optimizing cost/quality tradeoffs
# OLLAMA_TOPIC_MODEL=llama3.2:3b
# OLLAMA_DISCOURSE_MODEL=llama3.2:3b
# OLLAMA_STRUCTURE_MODEL=llama3.2:3b
# OLLAMA_TAG_MODEL=llama3.2:3b

# Temperature Settings (0.0-1.0, lower = more deterministic)
OLLAMA_TOPIC_TEMPERATURE=0.1         # Topic shift detection
OLLAMA_DISCOURSE_TEMPERATURE=0.1     # Discourse relationship classification
OLLAMA_STRUCTURE_TEMPERATURE=0.1     # Document structure detection
OLLAMA_TAG_TEMPERATURE=0.3           # Tag/entity extraction (higher for creativity)

# Boundary Detection Weights (must sum to ~1.0)
# Controls how different signals influence chunking decisions
SEMANTIC_WEIGHT_TOPIC=0.35           # Topic shift importance
SEMANTIC_WEIGHT_DISCOURSE=0.25       # Discourse relationship importance
SEMANTIC_WEIGHT_STRUCTURE=0.20       # Document structure importance
SEMANTIC_WEIGHT_SEMANTIC_EMBED=0.20  # Semantic embedding similarity importance

# Threshold Configuration (0.0-1.0)
SEMANTIC_BOUNDARY_THRESHOLD=0.6      # Minimum score to create boundary
SEMANTIC_CONFIDENCE_THRESHOLD=0.7    # Minimum analyzer confidence
SEMANTIC_SIMILARITY_THRESHOLD=0.75   # Semantic similarity threshold
