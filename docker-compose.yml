# Docker Compose - Arbiter Main Application Stack
# Core services: Qdrant, Ollama, MCP Server, Agent Orchestrator
# Start with: docker compose up -d
# CLI client is separate: docker compose -f docker-compose.cli.yml up

name: arbiter

networks:
  arbiter-network:
    driver: bridge
    name: arbiter-network

volumes:
  qdrant_storage:
    name: arbiter-qdrant-storage
  ollama_models:
    name: arbiter-ollama-models

services:
  # ===========================================================================
  # Qdrant Vector Database
  # ===========================================================================
  qdrant:
    image: qdrant/qdrant:v1.7.4
    container_name: arbiter-qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC API
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
    networks:
      - arbiter-network
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ===========================================================================
  # Ollama - Embeddings + LLM
  # ===========================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: arbiter-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
      - ./docker/scripts/ollama:/scripts
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-2}
      - OLLAMA_MAX_LOADED_MODELS=${OLLAMA_MAX_LOADED_MODELS:-2}
      - OLLAMA_FLASH_ATTENTION=${OLLAMA_FLASH_ATTENTION:-1}
      - CUDA_VISIBLE_DEVICES=0,1
      - OLLAMA_DEBUG=${OLLAMA_DEBUG:-false}
    logging:
      driver: "json-file"
      options:
        tag: "ollama"
        labels: "service"
    networks:
      - arbiter-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0', '1']
              capabilities: [gpu]
          memory: ${OLLAMA_MEMORY_RESERVATION:-8G}
        limits:
          memory: ${OLLAMA_MEMORY_LIMIT:-24G}

  # ===========================================================================
  # MCP Server - Context API
  # ===========================================================================
  mcp-server:
    build:
      context: .
      dockerfile: docker/services/Dockerfile.mcp-server
      target: production
    container_name: arbiter-mcp-server
    restart: unless-stopped
    ports:
      - "3100:3100"
    environment:
      # Override .env URLs for container-to-container communication
      - OLLAMA_BASE_URL=http://ollama:11434
      - QDRANT_URL=http://qdrant:6333
      - AGENT_ORCHESTRATOR_URL=${AGENT_ORCHESTRATOR_URL:-http://agent-orchestrator:3200}
      - TRANSPORT=${TRANSPORT:-streamable-http}
      - MCP_HTTP_PORT=${MCP_HTTP_PORT:-3100}
      - MAX_CONCURRENT_REQUESTS=${MAX_CONCURRENT_REQUESTS:-50}
      - REQUEST_TIMEOUT=${REQUEST_TIMEOUT:-30000}
      - NODE_ENV=${NODE_ENV:-production}
      - NODE_OPTIONS=${NODE_OPTIONS:---max-old-space-size=2048}
      - LOG_PREFIX=${LOG_PREFIX:-[ARBITER]}
      - LOG_FORMAT=${LOG_FORMAT:-text}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_USE_COLORS=${LOG_USE_COLORS:-true}
    depends_on:
      qdrant:
        condition: service_started
    networks:
      - arbiter-network
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3100/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ===========================================================================
  # Agent Orchestrator - Core Agent Logic
  # ===========================================================================
  agent-orchestrator:
    build:
      context: .
      dockerfile: docker/services/Dockerfile.orchestrator
      target: production
    container_name: arbiter-agent-orchestrator
    restart: unless-stopped
    ports:
      - "3200:3200"
    environment:
      # Override .env URLs for container-to-container communication
      - MCP_SERVER_URL=http://mcp-server:3100
      - OLLAMA_BASE_URL=http://ollama:11434
      - QDRANT_URL=http://qdrant:6333
      - OLLAMA_TIMEOUT=${OLLAMA_TIMEOUT:-180000}
      - LLM_MODEL=${LLM_MODEL:-llama3.1:8b}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-nomic-embed-text}
      - AGENT_PERSONALITY=${AGENT_PERSONALITY:-none}
      - QUERY_DECOMPOSER_MODEL=${QUERY_DECOMPOSER_MODEL}
      - QUERY_ENHANCER_MODEL=${QUERY_ENHANCER_MODEL}
      - RAG_VALIDATOR_MODEL=${RAG_VALIDATOR_MODEL}
      - QUALITY_GRADER_MODEL=${QUALITY_GRADER_MODEL}
      - TOOL_PLANNER_MODEL=${TOOL_PLANNER_MODEL}
      - QUERY_ROUTER_COMPLEXITY_THRESHOLD=${QUERY_ROUTER_COMPLEXITY_THRESHOLD:-5}
      - CONTEXT_MAX_TOKENS=${CONTEXT_MAX_TOKENS:-12288}
      - CONTEXT_MIN_RESPONSE_TOKENS=${CONTEXT_MIN_RESPONSE_TOKENS:-512}
      - CONTEXT_CHARS_PER_TOKEN=${CONTEXT_CHARS_PER_TOKEN:-4}
      - HYBRID_SEARCH_MAX_RESULTS=${HYBRID_SEARCH_MAX_RESULTS:-50}
      - RAG_VALIDATOR_MAX_PARALLEL=${RAG_VALIDATOR_MAX_PARALLEL:-5}
      - RAG_VALIDATOR_MIN_SCORE=${RAG_VALIDATOR_MIN_SCORE:-0.3}
      - ORCHESTRATOR_PORT=${ORCHESTRATOR_PORT:-3200}
      - NODE_ENV=${NODE_ENV:-production}
      - NODE_OPTIONS=${NODE_OPTIONS:---max-old-space-size=2048}
      - LOG_PREFIX=${LOG_PREFIX:-[ARBITER]}
      - LOG_FORMAT=${LOG_FORMAT:-text}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_USE_COLORS=${LOG_USE_COLORS:-true}
    depends_on:
      mcp-server:
        condition: service_healthy
      ollama:
        condition: service_started
    networks:
      arbiter-network:
        aliases:
          - agent-orchestrator
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3200/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ===========================================================================
  # Ollama Model Initialization
  # ===========================================================================
  ollama-init:
    image: ollama/ollama:latest
    container_name: arbiter-ollama-init
    depends_on:
      ollama:
        condition: service_started
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=ollama:11434
      - LLM_MODEL=${LLM_MODEL:-llama3.1:8b}
    entrypoint: /bin/sh
    command: >
      -c "
      echo 'Pulling required Ollama models...' &&
      ollama pull nomic-embed-text &&
      echo '✓ nomic-embed-text (embeddings) downloaded' &&
      ollama pull $${LLM_MODEL:-llama3.1:8b} &&
      echo '✓ $${LLM_MODEL:-llama3.1:8b} (primary LLM) downloaded' &&
      ollama pull qwen2.5:14b &&
      echo '✓ qwen2.5:14b (fallback) downloaded' &&
      ollama pull phi4:14b-q4_K_M &&
      echo '✓ phi4:14b-q4_K_M (fallback reasoning) downloaded' &&
      echo 'All models initialized successfully!'
      "
    networks:
      - arbiter-network
    restart: "no"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ===========================================================================
  # Ollama Model Warming (Always Enabled)
  # ===========================================================================
  ollama-warm:
    image: curlimages/curl:latest
    container_name: arbiter-ollama-warm
    depends_on:
      ollama-init:
        condition: service_completed_successfully
      ollama:
        condition: service_started
    volumes:
      - ./docker/scripts/ollama:/scripts:ro
    environment:
      - OLLAMA_HOST=ollama:11434
      - LLM_MODEL=${LLM_MODEL:-llama3.1:8b}
    entrypoint: /bin/sh
    command: /scripts/warm-models.sh
    networks:
      - arbiter-network
    restart: "no"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
