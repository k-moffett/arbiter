# =============================================================================
# Semantic Text Chunking Configuration
# =============================================================================
# This file configures the semantic chunking service for intelligent document
# segmentation. Semantic chunking uses NLM analysis to identify natural topic
# boundaries, resulting in more coherent chunks for RAG systems.
#
# Performance Impact:
# - Adaptive threshold: Balances quality vs speed
# - Higher candidate limits: Better quality, slower processing
# - Lower weights: Faster but less accurate
#
# Quick Start (Recommended):
# Copy this file to .env.text-chunking and use defaults for balanced performance

# =============================================================================
# Adaptive Threshold Settings
# =============================================================================
# Adaptive threshold automatically adjusts boundary detection based on document
# statistics. Recommended for most use cases.

# Enable adaptive threshold calculation (true = auto-adjust, false = use max threshold)
SEMANTIC_ADAPTIVE_THRESHOLD_ENABLED=true

# Minimum semantic distance to consider as boundary (0.0 - 1.0)
# Lower = more sensitive, catches subtle topic shifts
# Higher = less sensitive, only major topic changes
# Recommended: 0.3 for technical docs, 0.4 for general content
SEMANTIC_ADAPTIVE_THRESHOLD_MIN=0.3

# Maximum semantic distance threshold (0.0 - 1.0)
# Used when adaptive is disabled, or as upper bound when enabled
# Recommended: 0.8 (catches clear topic shifts)
SEMANTIC_ADAPTIVE_THRESHOLD_MAX=0.8

# Maximum number of candidate boundaries to analyze with LLM
# Higher = better quality but slower, Lower = faster but may miss boundaries
# Recommended: 500 for 400-page docs (results in ~3-5 min processing)
SEMANTIC_ADAPTIVE_CANDIDATE_LIMIT=500

# =============================================================================
# Boundary Score Weights
# =============================================================================
# These weights determine how different signals contribute to boundary detection.
# IMPORTANT: Must sum to 1.0 (validated at startup)

# Weight for embedding distance (fast, always calculated)
# Measures semantic similarity between sentences
SEMANTIC_WEIGHT_SEMANTIC_EMBED=0.30

# Weight for topic analyzer (LLM-based, only on candidates)
# Detects topic shifts using LLM analysis
SEMANTIC_WEIGHT_TOPIC=0.25

# Weight for discourse classifier (LLM-based, only on candidates)
# Analyzes discourse relationships (however, therefore, etc.)
SEMANTIC_WEIGHT_DISCOURSE=0.25

# Weight for structure detector (always calculated)
# Detects headers, lists, tables, and structural boundaries
SEMANTIC_WEIGHT_STRUCTURE=0.20

# =============================================================================
# Structure Preservation
# =============================================================================

# Always preserve atomic units (tables, lists, code blocks)
# true = never split tables/lists (recommended)
# false = allow splitting if needed (not recommended)
SEMANTIC_PRESERVE_STRUCTURE=true

# =============================================================================
# Progress Feedback
# =============================================================================

# Log progress every N sentences during boundary analysis
# Lower = more frequent updates (verbose), Higher = less frequent
# Recommended: 10 for good UX without log spam
SEMANTIC_PROGRESS_INTERVAL=10

# =============================================================================
# Chunking Parameters
# =============================================================================

# Minimum chunk size in characters
# Prevents overly small chunks that lack context
SEMANTIC_MIN_CHUNK_SIZE=300

# Maximum chunk size in characters
# Prevents chunks from exceeding context windows
# Note: Chunks may be force-split if they exceed this even at boundaries
SEMANTIC_MAX_CHUNK_SIZE=1500

# Target chunk size in characters
# Preferred size when no boundary constraints
SEMANTIC_TARGET_CHUNK_SIZE=1000

# Overlap between chunks as percentage (0-100)
# Provides context continuity between adjacent chunks
# Recommended: 15 (15% overlap)
SEMANTIC_OVERLAP_PERCENTAGE=15

# =============================================================================
# Tag Extraction
# =============================================================================

# Enable NLM-powered metadata extraction for chunks
# Extracts tags, entities, topics, key phrases using LLM
# true = extract metadata (recommended for RAG)
# false = skip extraction (faster but less searchable)
SEMANTIC_TAG_EXTRACTION_ENABLED=true

# Model to use for tag extraction (Ollama model name)
# Recommended: llama3.2:3b (good balance of speed/quality)
# Alternatives: llama3.1:8b (slower, better quality)
SEMANTIC_TAG_EXTRACTION_MODEL=llama3.2:3b

# Temperature for tag extraction (0.0 - 1.0)
# Lower = more deterministic, Higher = more creative
# Recommended: 0.2 (consistent extractions)
SEMANTIC_TAG_EXTRACTION_TEMPERATURE=0.2

# =============================================================================
# LLM Token Limits (Response Generation)
# =============================================================================
# These settings control how many tokens each LLM analyzer can generate.
# Token limits directly impact accuracy - if too low, responses get truncated
# before all required fields are generated, causing validation failures.
#
# Philosophy: ACCURACY > SPEED
# These generous defaults ensure ANY PDF can be processed without truncation,
# even if it takes 20-30 minutes. Lower values risk missing required fields.
#
# Minimum Safe Values (for reference, NOT recommended):
# - Structure: 150 (will fail on verbose documents)
# - Tag: 300 (may truncate on rich content)
# - Topic: 120 (may truncate reasoning)
# - Discourse: 150 (may truncate explanations)

# Structure Detector: Identifies tables, Q&A pairs, code blocks, lists
# Requires enough tokens for detailed structure descriptions
# Recommended: 500 (handles complex tables with multiple columns)
OLLAMA_STRUCTURE_NUM_PREDICT=500

# Tag Extractor: Extracts entities, topics, tags, key phrases
# Requires enough tokens for comprehensive metadata arrays
# Recommended: 500 (handles documents with rich terminology)
OLLAMA_TAG_NUM_PREDICT=500

# Topic Analyzer: Detects topic shifts between sentences
# Requires enough tokens for reasoning and confidence scores
# Recommended: 300 (handles nuanced topic analysis)
OLLAMA_TOPIC_NUM_PREDICT=300

# Discourse Classifier: Identifies rhetorical relationships
# Requires enough tokens for relationship explanations
# Recommended: 400 (handles complex discourse patterns)
OLLAMA_DISCOURSE_NUM_PREDICT=400

# =============================================================================
# Model Configuration
# =============================================================================
# Base Ollama model for all semantic chunking analyzers
# Recommended: llama3.1:8b (good balance of speed/quality, widely available)
OLLAMA_SEMANTIC_CHUNKER_MODEL=llama3.1:8b

# =============================================================================
# Performance Notes
# =============================================================================
# Expected processing time for 400-page PDF (~4000 sentences):
#
# With default settings:
# - Pass 1 (Embeddings): ~60-90 seconds
# - Pass 2 (Structure): ~30-60 seconds
# - Pass 3 (LLM Candidates): ~90-120 seconds (500 candidates Ã— 3 analyzers)
# - Tag Extraction: ~60-90 seconds (400 chunks)
# - Total: 3-5 minutes
#
# Optimization tips:
# - Reduce CANDIDATE_LIMIT for faster processing (may miss boundaries)
# - Increase MIN_THRESHOLD for fewer candidates (faster but less precise)
# - Disable TAG_EXTRACTION for embedding-only mode (2x faster)
#
# =============================================================================
# Development & Testing Overrides
# =============================================================================
# These settings can be uncommented for development/testing purposes
#
# SEMANTIC_ADAPTIVE_THRESHOLD_ENABLED=false  # Disable adaptive, use max threshold only
# SEMANTIC_ADAPTIVE_CANDIDATE_LIMIT=50       # Faster testing with fewer candidates
# SEMANTIC_TAG_EXTRACTION_ENABLED=false      # Skip tag extraction for speed testing
